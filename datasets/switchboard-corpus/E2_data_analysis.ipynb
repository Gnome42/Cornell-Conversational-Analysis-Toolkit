{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2: Applying Transformers to the Corpus\n",
    "\n",
    "This notebook is used to apply and analyze measures on the Switchboard corpus via a series of ConvoKit Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies and load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../../\") # import convokit\n",
    "from convokit import Corpus, User, Utterance\n",
    "os.chdir(\"datasets/switchboard-corpus\") # then come back for switchboard\n",
    "import convokit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the switchboard corpus\n",
    "corpus = convokit.Corpus(filename = \"./corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to use for tokenizing utterances\n",
    "def tokenize_utt(utterance: str):\n",
    "    # Strips punctuation from utterance and returns a list of tokens\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\t\n",
    "    tokens = tokenizer.tokenize(utterance)\n",
    "\n",
    "    # Remove any tokens that are a single letter other than 'I' or 'a'\n",
    "    # because they are tags for the utterance tree\n",
    "    for t in tokens:\n",
    "        if len(t) == 1 and not (t == 'I' or t == 'a'):\n",
    "            tokens.remove(t)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply all the transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<convokit.model.corpus.Corpus at 0x7fcc6c584cc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the ConversationBalance transformer\n",
    "cb = convokit.ConversationBalance()\n",
    "cb.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<convokit.model.corpus.Corpus at 0x7fcc6c584cc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the VocabOverlap transformer\n",
    "vo = convokit.VocabOverlap()\n",
    "vo.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the FirstImpression transformer\n",
    "fi = convokit.FirstImpression()\n",
    "fi.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the DAMSLScores transformer\n",
    "ds = convokit.DAMSLScores('../../convokit/damslScores/damsl_rubric.txt')\n",
    "ds.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure 1: Conversation Balance\n",
    "\n",
    "Let's confirm the changes to the corpus. The conversation balance is saved as an array of size NxN where N is the number of users. Cell (X,Y) in the array is the ratio of number of tokens said by User A to the total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(corpus.iter_conversations()):\n",
    "    balance = c.meta['conversation_balance']\n",
    "    print(i+1,')  Balance from User 1 to User 2:', balance[0,1])\n",
    "    \n",
    "    if i == 2:\n",
    "        break\n",
    "        \n",
    "print('We can see from this that the balance of convo 1 is the best because it is closest to 0.5')\n",
    "print('In conversation 2, we see that User 2 speaks twice as much as User 1 (0.33 = 1/(1+2))')\n",
    "print('Conversation 3 is similar to conversation 1 and is about equal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the balance within a conversation. Here, we will call each group of consecutive utterances by the same User a statement. The statement balance is stored in the first utterance of a statement. It tells us the ratio of tokens of the current statement to the sum of the current statement's tokens and the next statement's tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at just one conversation:\n",
    "print('Users in this conversation:', corpus.conversations['2095-0']._usernames)\n",
    "\n",
    "print('\\nHere are the IDs for the first 5 utterances of the conversation, and the balance from',\n",
    "     '\\nthis utterance to the next:')\n",
    "utt_list = corpus.conversations['2095-0']._utterance_ids\n",
    "for i, u in enumerate(utt_list):\n",
    "    if 'statement_balance' in corpus.utterances[u].meta:\n",
    "        print(u, corpus.utterances[u].meta['statement_balance'])\n",
    "        \n",
    "    if i > 4:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this conversation, let's see how the statement balance plots over the length of the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utt_list = corpus.conversations['2095-0']._utterance_ids\n",
    "statement_ids = []\n",
    "statement_balances = []\n",
    "\n",
    "for u in utt_list:\n",
    "    if 'statement_balance' in corpus.utterances[u].meta:\n",
    "        statement_ids.append(u)\n",
    "        statement_balances.append(corpus.utterances[u].meta['statement_balance'])\n",
    "    \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(len(statement_balances)), statement_balances, 'b*-')\n",
    "plt.plot(range(len(statement_balances)), 0.5*np.ones(len(statement_balances)), 'r:')\n",
    "plt.title('Balance over Conversation 2095')\n",
    "plt.xlabel('Statement Number')\n",
    "plt.ylabel('Balance of Current/Next Statement')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average balance over the first 10% of each conversation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg_balance = []\n",
    "rest_balance = []\n",
    "\n",
    "for c in corpus.conversations:\n",
    "    utt_list = corpus.conversations[c]._utterance_ids\n",
    "    statement_ids = []\n",
    "    statement_balances = []\n",
    "    \n",
    "    # Get list of statement balances\n",
    "    for u in utt_list:\n",
    "        if 'statement_balance' in corpus.utterances[u].meta:\n",
    "            statement_ids.append(u)\n",
    "            statement_balances.append(corpus.utterances[u].meta['statement_balance'])\n",
    "            \n",
    "    # Split up the statements to different parts of the conversation\n",
    "    split = round(0.1*len(statement_balances))\n",
    "\n",
    "    beginning = statement_balances[:split]\n",
    "    rest = statement_balances[split:]\n",
    "    assert(len(beginning)+len(rest) == len(statement_balances))\n",
    "    beg_balance.append(sum(beginning)/len(beginning))\n",
    "    rest_balance.append(sum(rest)/len(rest))\n",
    "        \n",
    "print('The average balance of the first 10% of the conversation is:', sum(beg_balance)/len(beg_balance))\n",
    "print('The average balance of the rest of the conversation is:', sum(rest_balance)/len(rest_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, it doesn't seem as though the first 10% of a conversation has a different balance than the rest. Let's look at the balance in other parts of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg_balance = []\n",
    "rest_balance = []\n",
    "\n",
    "for c in corpus.conversations:\n",
    "    utt_list = corpus.conversations[c]._utterance_ids\n",
    "    statement_ids = []\n",
    "    statement_balances = []\n",
    "    \n",
    "    # Get list of statement balances\n",
    "    for u in utt_list:\n",
    "        if 'statement_balance' in corpus.utterances[u].meta:\n",
    "            statement_ids.append(u)\n",
    "            statement_balances.append(corpus.utterances[u].meta['statement_balance'])\n",
    "\n",
    "    # Split up the statements to different parts of the conversation\n",
    "    beginning = statement_balances[:2]\n",
    "    rest = statement_balances[2:]\n",
    "    assert(len(beginning)+len(rest) == len(statement_balances))\n",
    "    beg_balance.append(sum(beginning)/len(beginning))\n",
    "    rest_balance.append(sum(rest)/len(rest))\n",
    "        \n",
    "print('The average balance of the first exchange of the conversation is:', sum(beg_balance)/len(beg_balance))\n",
    "print('The average balance of the rest of the conversation is:', sum(rest_balance)/len(rest_balance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg_balance = []\n",
    "mid_balance = []\n",
    "end_balance = []\n",
    "\n",
    "for c in corpus.conversations:\n",
    "    utt_list = corpus.conversations[c]._utterance_ids\n",
    "    statement_ids = []\n",
    "    statement_balances = []\n",
    "    \n",
    "    # Get list of statement balances\n",
    "    for u in utt_list:\n",
    "        if 'statement_balance' in corpus.utterances[u].meta:\n",
    "            statement_ids.append(u)\n",
    "            statement_balances.append(corpus.utterances[u].meta['statement_balance'])\n",
    "            \n",
    "    # Split up the statements to different parts of the conversation\n",
    "    split1 = round(0.2*len(statement_balances))\n",
    "    split2 = round(0.8*len(statement_balances))\n",
    "\n",
    "    beginning = statement_balances[:split1]\n",
    "    middle = statement_balances[split1:split2]\n",
    "    end = statement_balances[split2:]\n",
    "    \n",
    "    assert(len(beginning)+len(middle)+len(end) == len(statement_balances))\n",
    "    \n",
    "    beg_balance.append(sum(beginning)/len(beginning))\n",
    "    mid_balance.append(sum(middle)/len(middle))\n",
    "    end_balance.append(sum(end)/len(end))\n",
    "        \n",
    "print('The average balance of the beginning of the conversation is:', sum(beg_balance)/len(beg_balance))\n",
    "print('The average balance of the middle of the conversation is:', sum(mid_balance)/len(mid_balance))\n",
    "print('The average balance of the end of the conversation is:', sum(end_balance)/len(end_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also very little difference between the middle and end of the conversation. It may be that there is no general pattern in conversation balance in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure 2: Overlapping use of vocabulary\n",
    "\n",
    "Next we move onto use of overlapping vocabulary. Our hypothesis is that people tend to repeat what the other person says if the users are more engaged in the conversation. This measurement computes the proportion of tokens that are used by all users, while excluding stop words commonly used such as 'a', 'is', and 'or'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = []\n",
    "for i, convo in enumerate(corpus.iter_conversations()):\n",
    "    \n",
    "    overlaps.append(convo.meta['vocabulary_overlap']['ratio'])\n",
    "        \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(overlaps, bins = np.linspace(0,1,num=11))\n",
    "plt.title('Histogram of Overlap Ratios')\n",
    "plt.xlabel('Overlap')\n",
    "plt.ylabel('Number of conversations')\n",
    "plt.show()\n",
    "\n",
    "print('Mean = %f / StDev = %f' % (np.mean(overlaps), np.std(overlaps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram above, we see that the overlap ratio in conversations ranges within 0.3 to 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe what words are overlapping across different users\n",
    "for i, convo in enumerate(corpus.iter_conversations()):\n",
    "    \n",
    "    vo = convo.meta['vocabulary_overlap']\n",
    "    print('+ Overlapping words in convo %s: %s\\n' % (convo.id, vo['vocab']))\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using just stop words still leaves some commonly used words with less content. Also, repeated usage of rare words should be more significant for our purpose, and thus one extension from here would be to apply an additional filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate whether different topics of conversaition leads to\n",
    "# more or less overlapping vocabulary amongst the users\n",
    "topicCounts = {}\n",
    "topicOverlap = {}\n",
    "for conv_id in corpus.meta['metadata']:\n",
    "    \n",
    "    topic = corpus.meta['metadata'][conv_id]['topic_description']\n",
    "    \n",
    "    convo = corpus.conversations[conv_id + '-0']\n",
    "    overlap = convo.meta['vocabulary_overlap']['ratio']\n",
    "    \n",
    "    if topic not in topicCounts:\n",
    "        topicCounts[topic] = 1\n",
    "        topicOverlap[topic] = [overlap]\n",
    "    else:\n",
    "        topicCounts[topic] += 1\n",
    "        topicOverlap[topic].append(overlap)\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "for overlap in topicOverlap.values():\n",
    "    means.append(np.mean(overlap))\n",
    "    stds.append(np.std(overlap))\n",
    "        \n",
    "# Sort by the average vocab overlap\n",
    "idx = np.argsort(means)\n",
    "    \n",
    "# Plot politeness scores for each topic (error bar showing 1 standard deviation)\n",
    "x = np.arange(len(means))\n",
    "\n",
    "means = np.array(means)\n",
    "stds = np.array(stds)\n",
    "\n",
    "plt.rcdefaults()\n",
    "plt.figure(figsize=(15, 23))\n",
    "plt.errorbar(means[idx], x, xerr=stds[idx], linestyle='None', marker='o')\n",
    "plt.yticks(x, [list(topicOverlap.keys())[i] for i in idx])\n",
    "plt.grid()\n",
    "plt.title('Vocab Overlap vs. Topic of Conversation')\n",
    "plt.xlabel('Ratio')\n",
    "plt.ylabel('Topic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see an interesting trend in which easily accessible topics such as fishing, movies, and baseball show larger overlap within vocabularies, whereas topics requiring rather more knowledge such as soviet union, ethics in government, and air pollution show less overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure 3: Scores measured with DAMSL tags\n",
    "\n",
    "Among 43 different DAMSL act-tags shown in Table 3 of http://compprag.christopherpotts.net/swda.html, we believe some tags are indicative of engaging and useful interactions within conversations (e.g. acknowledgement or reformulation), whereas others indicate less-quality conversations (e.g. Uninterpretable or 3rd-party talk). \n",
    "\n",
    "By checking example utterances with each tag, we classified each tag by hand, after which each tag has a score based on its indication: +1 (good interaction), 0 (neutral), and -1 (bad interaction). This hand-crafted rubric can be found in `/convokit/damslScores/damsl_rubric.txt`. Out of 43 DAMSL tags, 21 tags are marked as +1, 8 tags as -1, and 14 tags as 0. The final DAMSL score stored in the conversation-level metadata under key `damsl_score` is simply the average of the scores across all utterances within the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i, convo in enumerate(corpus.iter_conversations()):\n",
    "    scores.append(convo.meta['damsl_score'])\n",
    "    \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(overlaps, bins = np.linspace(0,1,num=11))\n",
    "plt.title('Histogram of DAMSL Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Number of conversations')\n",
    "plt.show()\n",
    "\n",
    "print('Mean = %f / StDev = %f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all scores reside on the positive side, which is expected as we assigned half of the tags as +1, and a lot less as -1. A lot of frequent tags are also often tagged as +1, further pushing the average towards the positive side.\n",
    "\n",
    "Instead of hand-crafting the points naively as +1 or -1, it would be helpful to have a DAMSL-tagged dataset with human-annotated conversation quality scores, with which we can build a regression model to learn the suitable weights of each tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure 4: Sentiment analysis for first impressions\n",
    "\n",
    "Lastly, we perform sentiment analysis on the utterances that cover the first 10% of each conversation, to represent how good or bad the users' first impressions were. We use the pretrained NLTK VADER sentiment analyzer, in which there are four metrics:\n",
    "- `neg/neu/pos`: These represent the negative/neutral/positive sentiments of an utterance/conversation. Each of these range within [0,1], and the three sums up to 1.\n",
    "- `compound`: This aggregated score ranges from -1 (extreme negative) to +1 (extreme positive), and is computed by iterating through a lexicon with several heuristics. For further detail, please refer to http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf. We will mainly use this metric to check our hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out sample initial utterances for sanity-check\n",
    "i = 0\n",
    "for k, v in corpus.meta['metadata'].items():\n",
    "    \n",
    "    from_caller = v['from_caller']\n",
    "    to_caller = v['to_caller']\n",
    "    convo = corpus.conversations[k+'-0']\n",
    "    print('+ Conversation %s' % k)\n",
    "    print('- Polarity score of from-caller (%s) = %f' % (from_caller, convo.meta['first_impression'][from_caller]['compound']))\n",
    "    print('- Polarity score of to-caller   (%s) = %f' % (to_caller, convo.meta['first_impression'][to_caller]['compound']))\n",
    "    print('- Sample beginning text:')\n",
    "    for idx, utt in enumerate(convo.iter_utterances()):\n",
    "        tokens = tokenize_utt(utt.text)\n",
    "        if utt.user.name == from_caller:\n",
    "            print('  (FROM): %s' % ' '.join(tokens))\n",
    "        else:\n",
    "            print('  ( TO ): %s' % ' '.join(tokens))\n",
    "        if idx > 15:\n",
    "            break\n",
    "            \n",
    "    print('\\n')\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compoundsFrom = []\n",
    "compoundsTo = []\n",
    "for k, v in corpus.meta['metadata'].items():\n",
    "    \n",
    "    from_caller = v['from_caller']\n",
    "    to_caller = v['to_caller']\n",
    "    convo = corpus.conversations[k+'-0']\n",
    "    compoundsFrom.append(convo.meta['first_impression'][from_caller]['compound'])\n",
    "    compoundsTo.append(convo.meta['first_impression'][to_caller]['compound'])\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(compoundsFrom, bins = np.linspace(-1,1,num=11))\n",
    "plt.title('Histogram of Compound Scores of From-Callers')\n",
    "plt.xlabel('Compound Sentiment')\n",
    "plt.ylabel('Number of conversations')\n",
    "plt.ylim(bottom=0, top=550)\n",
    "plt.show()\n",
    "print('Mean = %f / StDev = %f' % (np.mean(compoundsFrom), np.std(compoundsFrom)))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(compoundsTo, bins = np.linspace(-1,1,num=11))\n",
    "plt.title('Histogram of Compound Scores of To-Callers')\n",
    "plt.xlabel('Compound Sentiment')\n",
    "plt.ylabel('Number of conversations')\n",
    "plt.ylim(bottom=0, top=550)\n",
    "plt.show()\n",
    "print('Mean = %f / StDev = %f' % (np.mean(compoundsTo), np.std(compoundsTo)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results show that the sentiment of to-callers have slightly more negative sentiments at the beginning of a conversation. This behavior is expected, as it is the from-caller's intention to call the recepient, and there may be situations where the to-caller would prefer not to start the whole conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking our main hypothesis\n",
    "\n",
    "We mainly hypothesized that the first impression starting a conversation is positively correlated to the quality of the entire conversation. If the caller starts the conversation with a kind introduction with a positive tone, we would expect the receiver to be more likely to respond in a similar manner and answer the caller's questions with more detail. Conversely, if the introduction is terse and abrupt, the receiver will be less likely to cooperate or to provide thought-out and detailed responses. In essence, we test to see if measure 4 (sentiment analysis) has any correlation with the other three measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Impression vs. Conversation Balance\n",
    "\n",
    "We first check the correlation between the initial sentiment and the conversation balance. We use the deviation of the balance term from 0.5 to measure how good a conversation was (i.e. an ideal conversation would have 0 deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convoBalances = []\n",
    "compoundsFrom = []\n",
    "compoundsTo = []\n",
    "\n",
    "for k, v in corpus.meta['metadata'].items():\n",
    "    \n",
    "    from_caller = v['from_caller']\n",
    "    to_caller = v['to_caller']\n",
    "    convo = corpus.conversations[k+'-0']\n",
    "    \n",
    "    # We measure how far the conversation balance is far from 50-50\n",
    "    convoBalances.append(abs(0.5-convo.meta['conversation_balance'][0,1]))\n",
    "    \n",
    "    compoundsFrom.append(convo.meta['first_impression'][from_caller]['compound'])\n",
    "    compoundsTo.append(convo.meta['first_impression'][to_caller]['compound'])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(compoundsFrom, convoBalances)\n",
    "plt.title('Compound Scores of From-Caller vs. Conversation Balance')\n",
    "plt.xlabel('Compound Sentiment')\n",
    "plt.ylabel('Conversation Balance')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(compoundsTo, convoBalances)\n",
    "plt.title('Compound Scores of To-Caller vs. Conversation Balance')\n",
    "plt.xlabel('Compound Sentiment')\n",
    "plt.ylabel('Conversation Balance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Impression vs. Overlapping use of vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlaps = []\n",
    "compoundsFrom = []\n",
    "compoundsTo = []\n",
    "\n",
    "for k, v in corpus.meta['metadata'].items():\n",
    "    \n",
    "    from_caller = v['from_caller']\n",
    "    to_caller = v['to_caller']\n",
    "    convo = corpus.conversations[k+'-0']\n",
    "    \n",
    "    overlaps.append(convo.meta['vocabulary_overlap']['ratio'])\n",
    "    \n",
    "    compoundsFrom.append(convo.meta['first_impression'][from_caller]['compound'])\n",
    "    compoundsTo.append(convo.meta['first_impression'][to_caller]['compound'])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(compoundsFrom, overlaps)\n",
    "plt.title('Compound Scores of From-Caller vs. Vocab Overlap')\n",
    "plt.xlabel('Compound Sentiment')\n",
    "plt.ylabel('Vocab Overlap')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(compoundsTo, overlaps)\n",
    "plt.title('Compound Scores of To-Caller vs. Vocab Overlap')\n",
    "plt.xlabel('Compound Sentiment')\n",
    "plt.ylabel('Vocab Overlap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First impression vs. DAMSL act-tag scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "compoundsFrom = []\n",
    "compoundsTo = []\n",
    "\n",
    "for k, v in corpus.meta['metadata'].items():\n",
    "    \n",
    "    from_caller = v['from_caller']\n",
    "    to_caller = v['to_caller']\n",
    "    convo = corpus.conversations[k+'-0']\n",
    "    \n",
    "    scores.append(convo.meta['damsl_score'])\n",
    "    \n",
    "    compoundsFrom.append(convo.meta['first_impression'][from_caller]['compound'])\n",
    "    compoundsTo.append(convo.meta['first_impression'][to_caller]['compound'])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(compoundsFrom, scores)\n",
    "plt.title('Compound Scores of From-Caller vs. Vocab Overlap')\n",
    "plt.xlabel('Compound Sentiment')\n",
    "plt.ylabel('Vocab Overlap')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(compoundsTo, scores)\n",
    "plt.title('Compound Scores of To-Caller vs. Vocab Overlap')\n",
    "plt.xlabel('Compound Sentiment')\n",
    "plt.ylabel('Vocab Overlap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
